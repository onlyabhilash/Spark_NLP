{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_02_Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0xcX0q/Qht7CMkCEHmB/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlyabhilash/Spark_NLP/blob/main/spart-nlp_basics/spark_02_Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDjbxN12XJsQ",
        "outputId": "8a6191d0-9b95-4a13-9ee4-6bba1e390d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "! pip install --ignore-installed -q spark-nlp==2.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mrq3BAVaF7Y",
        "outputId": "9aa921c8-8443-4d87-fa47-7def0fbfdf66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n",
            "\u001b[K     |████████████████████████████████| 215.7 MB 60 kB/s \n",
            "\u001b[K     |████████████████████████████████| 197 kB 20.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 138 kB 8.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "# params =>> gpu=False, spark23=False (start with spark 2.3)\n",
        "\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "-bH4eS3Egauh",
        "outputId": "347cd918-7d94-493b-b457-ec2874d58b86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 2.7.1\n",
            "Apache Spark version: 2.4.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f749eff0d90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://af8e140695d3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Spark Dataframe"
      ],
      "metadata": {
        "id": "v5-X2gRgayN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Peter Parker is a nice guy and lives in New York'\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF('text')\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbUFQhRPay-C",
        "outputId": "6887334b-87e7-4b47-83d1-54ef14e43c3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------+\n",
            "|text                                            |\n",
            "+------------------------------------------------+\n",
            "|Peter Parker is a nice guy and lives in New York|\n",
            "+------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "# if you want to create a spark datafarme from a list of strings\n",
        "\n",
        "text_list = ['Peter Parker is a nice guy and lives in New York.', 'Bruce Wayne is also a nice guy and lives in Gotham City.']\n",
        "\n",
        "spark.createDataFrame(text_list, StringType()).toDF(\"text\").show(truncate=80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A23oJUokbE5i",
        "outputId": "440e5dc3-7936-492e-c425-b5bc3537622f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "|                                                    text|\n",
            "+--------------------------------------------------------+\n",
            "|       Peter Parker is a nice guy and lives in New York.|\n",
            "|Bruce Wayne is also a nice guy and lives in Gotham City.|\n",
            "+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row \n",
        "\n",
        "spark.createDataFrame(list(map(lambda x : Row(text = x),text_list))).show(truncate = 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWIQlTSccy_I",
        "outputId": "db4c6a0f-c24b-481e-cce2-54768ed228a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "|                                                    text|\n",
            "+--------------------------------------------------------+\n",
            "|       Peter Parker is a nice guy and lives in New York.|\n",
            "|Bruce Wayne is also a nice guy and lives in Gotham City.|\n",
            "+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt"
      ],
      "metadata": {
        "id": "WoXuw7OLdxEn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./sample-sentences-en.txt') as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW78ZPqje-K_",
        "outputId": "b989ffa5-e9e9-43a0-c67f-49e87d5a3b8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peter is a very good person.\n",
            "My life in Russia is very interesting.\n",
            "John and Peter are brothers. However they don't support each other that much.\n",
            "Lucas Nogal Dunbercker is no longer happy. He has a good car though.\n",
            "Europe is very culture rich. There are huge churches! and big houses!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANffhapHfKe2",
        "outputId": "6c53826c-e0a8-477f-be03-338cf8179c96"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.select('text').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d8nUt0-fd0e",
        "outputId": "a2dabed9-4c9a-4cfd-ed2e-aab4dee46d25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFiles = spark.sparkContext.wholeTextFiles(\"./*.txt\",4)\n",
        "    \n",
        "spark_df_folder = textFiles.toDF(schema=['path','text'])\n",
        "\n",
        "spark_df_folder.show(truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dONTGXitfoF8",
        "outputId": "4b61d342-a541-408d-e123-0a9b3ece79ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+------------------------------+\n",
            "|                          path|                          text|\n",
            "+------------------------------+------------------------------+\n",
            "|file:/content/sample-senten...|Peter is a very good person...|\n",
            "+------------------------------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df_folder.select('text').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggmxb6ONlL5O",
        "outputId": "62fec4e6-5c15-427b-fbd6-e2fe4f56d48d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text=\"Peter is a very good person.\\nMy life in Russia is very interesting.\\nJohn and Peter are brothers. However they don't support each other that much.\\nLucas Nogal Dunbercker is no longer happy. He has a good car though.\\nEurope is very culture rich. There are huge churches! and big houses!\")]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\\\n",
        ".setCleanupMode(\"shrink\")\n",
        "\n",
        "doc_df = documentAssembler.transform(spark_df)\n",
        "\n",
        "doc_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0yZ1k-zlgcC",
        "outputId": "e83acd78-feee-43d2-d6c3-12e20849fa58"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QBkLnxQpXx_",
        "outputId": "4460f778-a608-4a28-916e-dbd16d64c8fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df.select('document.result','document.begin','document.end').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P2bADPCqOOd",
        "outputId": "b82edace-6093-4ee6-f2c0-ef658118f356"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|result                                                                         |begin|end |\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|[Peter is a very good person.]                                                 |[0]  |[27]|\n",
            "|[My life in Russia is very interesting.]                                       |[0]  |[37]|\n",
            "|[John and Peter are brothers. However they don't support each other that much.]|[0]  |[76]|\n",
            "|[Lucas Nogal Dunbercker is no longer happy. He has a good car though.]         |[0]  |[67]|\n",
            "|[Europe is very culture rich. There are huge churches! and big houses!]        |[0]  |[68]|\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df.select(\"document.result\").take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JslRDeSLq_Be",
        "outputId": "873b4842-7512-464d-f4b1-fb22f345889b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.'])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "doc_df.withColumn(\n",
        "    'tmp',\n",
        "    F.explode('document'))\\\n",
        "    .select('tmp.*')\\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIvlVEq-tHxO",
        "outputId": "2525ce76-3b94-475c-dcb9-d0b0712474c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|annotatorType|begin|end|result                                                                       |metadata       |embeddings|\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|document     |0    |27 |Peter is a very good person.                                                 |[sentence -> 0]|[]        |\n",
            "|document     |0    |37 |My life in Russia is very interesting.                                       |[sentence -> 0]|[]        |\n",
            "|document     |0    |76 |John and Peter are brothers. However they don't support each other that much.|[sentence -> 0]|[]        |\n",
            "|document     |0    |67 |Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[sentence -> 0]|[]        |\n",
            "|document     |0    |68 |Europe is very culture rich. There are huge churches! and big houses!        |[sentence -> 0]|[]        |\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Detector"
      ],
      "metadata": {
        "id": "IPrTj6KPvHZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        ".setInputCols(['document'])\\\n",
        ".setOutputCol('sentences')\n",
        "\n",
        "sentenceDetector.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDkDOOe6uEXT",
        "outputId": "1ded1856-4c76-4b35-ecae-2d8e9054cbda"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='SentenceDetector_1d0d527b4444', name='customBounds', doc='characters used to explicitly mark sentence bounds'): [],\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='detectLists', doc='whether detect lists during sentence detection'): True,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='explodeSentences', doc='whether to explode each sentence into a different row, for better parallelization. Defaults to false.'): False,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='maxLength', doc='Set the maximum allowed length for each sentence'): 99999,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='minLength', doc='Set the minimum allowed length for each sentence.'): 0,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='outputCol', doc='output annotation column. can be left default.'): 'sentences',\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='useAbbreviations', doc='whether to apply abbreviations at sentence detection'): True,\n",
              " Param(parent='SentenceDetector_1d0d527b4444', name='useCustomBoundsOnly', doc='Only utilize custom bounds in sentence detection'): False}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlml2MmPvmjw",
        "outputId": "df72f708-070d-4604-c000-7f48abe0dea5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |sentences                                                                                                                                                                                          |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                                                                                             |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                                                                                                   |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|[[document, 0, 27, John and Peter are brothers., [sentence -> 0], []], [document, 29, 76, However they don't support each other that much., [sentence -> 1], []]]                                  |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |[[document, 0, 41, Lucas Nogal Dunbercker is no longer happy., [sentence -> 0], []], [document, 43, 67, He has a good car though., [sentence -> 1], []]]                                           |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |[[document, 0, 27, Europe is very culture rich., [sentence -> 0], []], [document, 29, 52, There are huge churches!, [sentence -> 1], []], [document, 54, 68, and big houses!, [sentence -> 2], []]]|\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l04naSdKwM_K",
        "outputId": "fd1bb4ba-4704-4ef1-baef-cf49a3badd73"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='Peter is a very good person.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=37, result='My life in Russia is very interesting.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='John and Peter are brothers.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='document', begin=29, end=76, result=\"However they don't support each other that much.\", metadata={'sentence': '1'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text ='The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'\n",
        "text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Fvovbl6GyDgn",
        "outputId": "cbac13e1-a596-47f2-f852-91bd09c4150e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou3UhsZXz0tG",
        "outputId": "fb725093-5082-4026-cc0c-c05b61112dfd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                           |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = documentAssembler.transform(spark_df)\n",
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysG4lG_Mz9qo",
        "outputId": "2e165857-3a10-4678-8771-134742e0dcc6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                           |document                                                                                                                                                                                                                                                                                                                                                                                  |sentences                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.|[[document, 0, 334, The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months., [sentence -> 0], []]]|[[document, 0, 56, The patient was prescribed 1 capsule of Advil for 5 days., [sentence -> 0], []], [document, 58, 240, He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day., [sentence -> 1], []], [document, 242, 334, It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months., [sentence -> 2], []]]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU5ziUuU0c_G",
        "outputId": "77f725d4-b9a3-42b1-a2fb-7f36898af0d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['The patient was prescribed 1 capsule of Advil for 5 days.', 'He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.', 'It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceDetector.setExplodeSentences(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQhylLbo0pfJ",
        "outputId": "8e642efc-4cea-4efe-ce8e-886e958bdde3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceDetector_1d0d527b4444"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "\n",
        "sent_df.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sviCHl6m08Ux",
        "outputId": "104eb7b5-a493-4741-98fb-9856d916da47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                                          document|                                         sentences|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil f...|[[document, 0, 334, The patient was prescribed ...|[[document, 0, 56, The patient was prescribed 1...|\n",
            "|The patient was prescribed 1 capsule of Advil f...|[[document, 0, 334, The patient was prescribed ...|[[document, 58, 240, He was seen by the endocri...|\n",
            "|The patient was prescribed 1 capsule of Advil f...|[[document, 0, 334, The patient was prescribed ...|[[document, 242, 334, It was determined that al...|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6SBLg_m0-gD",
        "outputId": "696aec26-d3f7-4a19-fc28-2c3086d9a832"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[The patient was prescribed 1 capsule of Advil for 5 days.]                                                                                                                              |\n",
            "|[He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.]|\n",
            "|[It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.]                                                                                          |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select(F.explode('sentences.result')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W90PdTnz1C-W",
        "outputId": "5a344fb5-3cb8-44e9-f201-686ce5672ce2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|col                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
            "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
            "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Detector DL"
      ],
      "metadata": {
        "id": "pQNNPA5A4heC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencerDL = SentenceDetectorDLModel().pretrained('sentence_detector_dl',lang = 'en')\\\n",
        ".setInputCols(['document'])\\\n",
        ".setOutputCol('sentences')\n",
        "\n",
        "sent_dl_df = sentencerDL.transform(doc_df)\n",
        "sent_dl_df.select(F.explode('sentences.result')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W4p99R82h4J",
        "outputId": "d32693c4-548f-4611-888f-7c5868f2c71b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|col                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
            "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
            "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentences')\n",
        "    \n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentences\")\n",
        "\n",
        "sd_pipeline = PipelineModel(stages = [documenter,sentenceDetector])\n",
        "sd_model = LightPipeline(sd_pipeline)\n",
        "\n",
        "#DL version\n",
        "sd_dl_pipeline = PipelineModel(stages=[documenter, sentencerDL])\n",
        "\n",
        "sd_dl_model = LightPipeline(sd_dl_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5_Lw0yE60JN",
        "outputId": "53013bdf-afaf-4ce9-e230-37ec46704921"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"John loves Mary.Mary loves Peter\n",
        "Peter loves Helen .Helen loves John; \n",
        "Total: four people involved.\"\"\"\n",
        "\n",
        "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "  print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHohKznhbO2",
        "outputId": "7ebe21bd-a29a-4f28-f450-da6996e83a7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t51\tJohn loves Mary.Mary loves Peter\n",
            "Peter loves Helen .\n",
            "1\t52\t68\tHelen loves John;\n",
            "2\t71\t98\tTotal: four people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for anno in sd_dl_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2QejZYSnD2d",
        "outputId": "9d81d8f8-ab01-4530-b10e-fa17176f5dac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t15\tJohn loves Mary.\n",
            "1\t16\t32\tMary loves Peter\n",
            "2\t33\t51\tPeter loves Helen .\n",
            "3\t52\t68\tHelen loves John;\n",
            "4\t71\t98\tTotal: four people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "kvLGod2JoynG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "5vnWN0HRnq8T"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pfvQb3Lp-ws",
        "outputId": "7e78b5d6-7c62-407b-9263-7d489ef4dd87"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='Tokenizer_390875b0c691', name='caseSensitiveExceptions', doc='Whether to care for case sensitiveness in exceptions'): True,\n",
              " Param(parent='Tokenizer_390875b0c691', name='contextChars', doc='character list used to separate from token boundaries'): ['.',\n",
              "  ',',\n",
              "  ';',\n",
              "  ':',\n",
              "  '!',\n",
              "  '?',\n",
              "  '*',\n",
              "  '-',\n",
              "  '(',\n",
              "  ')',\n",
              "  '\"',\n",
              "  \"'\"],\n",
              " Param(parent='Tokenizer_390875b0c691', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='Tokenizer_390875b0c691', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='Tokenizer_390875b0c691', name='maxLength', doc='Set the maximum allowed legth for each token'): 99999,\n",
              " Param(parent='Tokenizer_390875b0c691', name='minLength', doc='Set the minimum allowed legth for each token'): 0,\n",
              " Param(parent='Tokenizer_390875b0c691', name='outputCol', doc='output annotation column. can be left default.'): 'token',\n",
              " Param(parent='Tokenizer_390875b0c691', name='targetPattern', doc='pattern to grab from text as token candidates. Defaults \\\\S+'): '\\\\S+'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbMv5l_qCao",
        "outputId": "54035d5a-f019-4aae-e912-bc3ce170b593"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------+\n",
            "|text                                                                           |\n",
            "+-------------------------------------------------------------------------------+\n",
            "|Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!|\n",
            "+-------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = documentAssembler.transform(spark_df)\n",
        "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
        "token_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I1pWwr8qWDG",
        "outputId": "0bee0114-e872-4aae-b570-cf01cfc990e9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                           |document                                                                                                                 |token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
            "+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!|[[document, 0, 78, Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!, [sentence -> 0], []]]|[[token, 0, 4, Peter, [sentence -> 0], []], [token, 6, 11, Parker, [sentence -> 0], []], [token, 13, 13, (, [sentence -> 0], []], [token, 14, 22, Spiderman, [sentence -> 0], []], [token, 23, 23, ), [sentence -> 0], []], [token, 25, 26, is, [sentence -> 0], []], [token, 28, 28, a, [sentence -> 0], []], [token, 30, 33, nice, [sentence -> 0], []], [token, 35, 37, guy, [sentence -> 0], []], [token, 39, 41, and, [sentence -> 0], []], [token, 43, 47, lives, [sentence -> 0], []], [token, 49, 50, in, [sentence -> 0], []], [token, 52, 54, New, [sentence -> 0], []], [token, 56, 59, York, [sentence -> 0], []], [token, 61, 63, but, [sentence -> 0], []], [token, 65, 67, has, [sentence -> 0], []], [token, 69, 70, no, [sentence -> 0], []], [token, 72, 77, e-mail, [sentence -> 0], []], [token, 78, 78, !, [sentence -> 0], []]]|\n",
            "+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_df.select('token.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtO_uPCrquWH",
        "outputId": "8127a5b3-4644-4cd9-9b7a-e0042eabcc05"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(', 'Spiderman', ')', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New', 'York', 'but', 'has', 'no', 'e-mail', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\") \\\n",
        "    .setSplitChars(['-']) \\\n",
        "    .setContextChars(['?', '!']) \\\n",
        "    .addException(\"New York\") \\\n",
        "\n",
        "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
        "\n",
        "token_df.select('token.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJnTIwEgq67v",
        "outputId": "b7812b9b-d179-415e-ea0c-f0eba5ae0355"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e', 'mail', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regex tokenizer"
      ],
      "metadata": {
        "id": "mkFr-V-Lzs5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%\"\n",
        "pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]])|(?<=[-.:;*+,$&%\\\\[\\\\]])\"\n",
        "\n",
        "df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n",
        "\n",
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "regexTokenizer = RegexTokenizer() \\\n",
        "      .setInputCols([\"sentence\"]) \\\n",
        "      .setOutputCol(\"regexToken\") \\\n",
        "      .setPattern(pattern) \\\n",
        "      .setPositionalMask(False)\n",
        "\n",
        "docPatternRemoverPipeline = \\\n",
        "  Pipeline() \\\n",
        "    .setStages([\n",
        "        documenter,\n",
        "        sentenceDetector,\n",
        "        regexTokenizer])\n",
        "    \n",
        "result = docPatternRemoverPipeline.fit(df).transform(df)\n",
        "\n",
        "result.show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdwuJ42zmkU",
        "outputId": "62bd4076-0b19-4341-c811-ee2e70a16da1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                               |document                                                                                     |sentence                                                                                     |regexToken                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%|[[document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, [sentence -> 0], []]]|[[document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, [sentence -> 0], []]]|[[token, 0, 0, 1, [sentence -> 0], []], [token, 2, 2, ., [sentence -> 0], []], [token, 4, 5, T1, [sentence -> 0], []], [token, 7, 7, -, [sentence -> 0], []], [token, 9, 10, T2, [sentence -> 0], []], [token, 12, 15, DATE, [sentence -> 0], []], [token, 17, 17, *, [sentence -> 0], []], [token, 19, 19, *, [sentence -> 0], []], [token, 21, 21, [, [sentence -> 0], []], [token, 23, 30, 12/24/13, [sentence -> 0], []], [token, 32, 32, ], [sentence -> 0], []], [token, 35, 35, $, [sentence -> 0], []], [token, 37, 37, 1, [sentence -> 0], []], [token, 39, 39, ., [sentence -> 0], []], [token, 41, 42, 99, [sentence -> 0], []], [token, 44, 45, (), [sentence -> 0], []], [token, 47, 53, (10/12), [sentence -> 0], []], [token, 55, 55, ,, [sentence -> 0], []], [token, 57, 58, ph, [sentence -> 0], []], [token, 60, 60, +, [sentence -> 0], []], [token, 62, 63, 90, [sentence -> 0], []], [token, 65, 65, %, [sentence -> 0], []]]|\n",
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result.select(F.explode('regexToken.result').alias('regexToken')).toPandas()\n",
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3Bm7k6Px0OgO",
        "outputId": "0640fb2a-d69f-4b02-e8cd-52d3762b6dcd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   regexToken\n",
              "0           1\n",
              "1           .\n",
              "2          T1\n",
              "3           -\n",
              "4          T2\n",
              "5        DATE\n",
              "6           *\n",
              "7           *\n",
              "8           [\n",
              "9    12/24/13\n",
              "10          ]\n",
              "11          $\n",
              "12          1\n",
              "13          .\n",
              "14         99\n",
              "15         ()\n",
              "16    (10/12)\n",
              "17          ,\n",
              "18         ph\n",
              "19          +\n",
              "20         90\n",
              "21          %"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6578cf39-cdf0-428d-bd39-bf46a75ba916\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>regexToken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12/24/13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>()</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(10/12)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6578cf39-cdf0-428d-bd39-bf46a75ba916')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6578cf39-cdf0-428d-bd39-bf46a75ba916 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6578cf39-cdf0-428d-bd39-bf46a75ba916');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking Spark NLP Annotators in Spark ML Pipeline"
      ],
      "metadata": {
        "id": "cZqnth_y1_Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector().\\\n",
        "setInputCols(['document']).\\\n",
        "setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentences\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " sentenceDetector,\n",
        " tokenizer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "PeaDDdDz02xJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
        "spark_df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCh75Lo2ppU",
        "outputId": "4d11f1b5-52f7-4f56-ba44-abd45b71d5cd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "result.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t_p5kCk27qi",
        "outputId": "560de1bd-7b08-40c9-e7b3-d30d197da636"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |sentences                                                                                                                                                                                          |token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                                                                                             |[[token, 0, 4, Peter, [sentence -> 0], []], [token, 6, 7, is, [sentence -> 0], []], [token, 9, 9, a, [sentence -> 0], []], [token, 11, 14, very, [sentence -> 0], []], [token, 16, 19, good, [sentence -> 0], []], [token, 21, 26, person, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []]]                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                                                                                                   |[[token, 0, 1, My, [sentence -> 0], []], [token, 3, 6, life, [sentence -> 0], []], [token, 8, 9, in, [sentence -> 0], []], [token, 11, 16, Russia, [sentence -> 0], []], [token, 18, 19, is, [sentence -> 0], []], [token, 21, 24, very, [sentence -> 0], []], [token, 26, 36, interesting, [sentence -> 0], []], [token, 37, 37, ., [sentence -> 0], []]]                                                                                                                                                                                                                                                                                                                          |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|[[document, 0, 27, John and Peter are brothers., [sentence -> 0], []], [document, 29, 76, However they don't support each other that much., [sentence -> 1], []]]                                  |[[token, 0, 3, John, [sentence -> 0], []], [token, 5, 7, and, [sentence -> 0], []], [token, 9, 13, Peter, [sentence -> 0], []], [token, 15, 17, are, [sentence -> 0], []], [token, 19, 26, brothers, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []], [token, 29, 35, However, [sentence -> 1], []], [token, 37, 40, they, [sentence -> 1], []], [token, 42, 46, don't, [sentence -> 1], []], [token, 48, 54, support, [sentence -> 1], []], [token, 56, 59, each, [sentence -> 1], []], [token, 61, 65, other, [sentence -> 1], []], [token, 67, 70, that, [sentence -> 1], []], [token, 72, 75, much, [sentence -> 1], []], [token, 76, 76, ., [sentence -> 1], []]]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |[[document, 0, 41, Lucas Nogal Dunbercker is no longer happy., [sentence -> 0], []], [document, 43, 67, He has a good car though., [sentence -> 1], []]]                                           |[[token, 0, 4, Lucas, [sentence -> 0], []], [token, 6, 10, Nogal, [sentence -> 0], []], [token, 12, 21, Dunbercker, [sentence -> 0], []], [token, 23, 24, is, [sentence -> 0], []], [token, 26, 27, no, [sentence -> 0], []], [token, 29, 34, longer, [sentence -> 0], []], [token, 36, 40, happy, [sentence -> 0], []], [token, 41, 41, ., [sentence -> 0], []], [token, 43, 44, He, [sentence -> 1], []], [token, 46, 48, has, [sentence -> 1], []], [token, 50, 50, a, [sentence -> 1], []], [token, 52, 55, good, [sentence -> 1], []], [token, 57, 59, car, [sentence -> 1], []], [token, 61, 66, though, [sentence -> 1], []], [token, 67, 67, ., [sentence -> 1], []]]       |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |[[document, 0, 27, Europe is very culture rich., [sentence -> 0], []], [document, 29, 52, There are huge churches!, [sentence -> 1], []], [document, 54, 68, and big houses!, [sentence -> 2], []]]|[[token, 0, 5, Europe, [sentence -> 0], []], [token, 7, 8, is, [sentence -> 0], []], [token, 10, 13, very, [sentence -> 0], []], [token, 15, 21, culture, [sentence -> 0], []], [token, 23, 26, rich, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []], [token, 29, 33, There, [sentence -> 1], []], [token, 35, 37, are, [sentence -> 1], []], [token, 39, 42, huge, [sentence -> 1], []], [token, 44, 51, churches, [sentence -> 1], []], [token, 52, 52, !, [sentence -> 1], []], [token, 54, 56, and, [sentence -> 2], []], [token, 58, 60, big, [sentence -> 2], []], [token, 62, 67, houses, [sentence -> 2], []], [token, 68, 68, !, [sentence -> 2], []]]      |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwByQx5o4auO",
        "outputId": "9bc8aa4e-d2e9-4a7b-f08e-697694ff205f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- sentences: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('sentences.result').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHRjD3oM4lL5",
        "outputId": "6cd1057d-5bc2-4962-e4da-d86724866f80"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.']),\n",
              " Row(result=['My life in Russia is very interesting.']),\n",
              " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token').take(3)[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33QKkCD34yUJ",
        "outputId": "481ad9d4-b6ec-40ac-9014-003d28784540"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(token=[Row(annotatorType='token', begin=0, end=3, result='John', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='However', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=42, end=46, result=\"don't\", metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=76, end=76, result='.', metadata={'sentence': '1'}, embeddings=[])])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizer"
      ],
      "metadata": {
        "id": "AvxbUWJO5pxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c-aEtxBN5gEX",
        "outputId": "7399b2a8-fe70-462a-a329-c170c5b8a252"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "    \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(True)\\\n",
        "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
        "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " normalizer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "ofuX266o5vo2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(spark_df)"
      ],
      "metadata": {
        "id": "0fidKvQi6DbM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOvb9NLP6E3A",
        "outputId": "d54e235a-9825-4c6a-c5d9-164c9cfb5c59"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |normalized                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |[[token, 0, 4, Peter, [sentence -> 0], []], [token, 6, 7, is, [sentence -> 0], []], [token, 9, 9, a, [sentence -> 0], []], [token, 11, 14, very, [sentence -> 0], []], [token, 16, 19, good, [sentence -> 0], []], [token, 21, 26, person, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []]]                                                                                                                                                                                                                                                                                                                                                                           |[[token, 0, 4, peter, [sentence -> 0], []], [token, 6, 7, is, [sentence -> 0], []], [token, 9, 9, a, [sentence -> 0], []], [token, 11, 14, very, [sentence -> 0], []], [token, 16, 19, good, [sentence -> 0], []], [token, 21, 26, person, [sentence -> 0], []]]                                                                                                                                                                                                                                                                                                                                 |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |[[token, 0, 1, My, [sentence -> 0], []], [token, 3, 6, life, [sentence -> 0], []], [token, 8, 9, in, [sentence -> 0], []], [token, 11, 16, Russia, [sentence -> 0], []], [token, 18, 19, is, [sentence -> 0], []], [token, 21, 24, very, [sentence -> 0], []], [token, 26, 36, interesting, [sentence -> 0], []], [token, 37, 37, ., [sentence -> 0], []]]                                                                                                                                                                                                                                                                                                                          |[[token, 0, 1, my, [sentence -> 0], []], [token, 3, 6, life, [sentence -> 0], []], [token, 8, 9, in, [sentence -> 0], []], [token, 11, 16, russia, [sentence -> 0], []], [token, 18, 19, is, [sentence -> 0], []], [token, 21, 24, very, [sentence -> 0], []], [token, 26, 36, interesting, [sentence -> 0], []]]                                                                                                                                                                                                                                                                                |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|[[token, 0, 3, John, [sentence -> 0], []], [token, 5, 7, and, [sentence -> 0], []], [token, 9, 13, Peter, [sentence -> 0], []], [token, 15, 17, are, [sentence -> 0], []], [token, 19, 26, brothers, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []], [token, 29, 35, However, [sentence -> 0], []], [token, 37, 40, they, [sentence -> 0], []], [token, 42, 46, don't, [sentence -> 0], []], [token, 48, 54, support, [sentence -> 0], []], [token, 56, 59, each, [sentence -> 0], []], [token, 61, 65, other, [sentence -> 0], []], [token, 67, 70, that, [sentence -> 0], []], [token, 72, 75, much, [sentence -> 0], []], [token, 76, 76, ., [sentence -> 0], []]]|[[token, 0, 3, john, [sentence -> 0], []], [token, 5, 7, and, [sentence -> 0], []], [token, 9, 13, peter, [sentence -> 0], []], [token, 15, 17, are, [sentence -> 0], []], [token, 19, 26, brothers, [sentence -> 0], []], [token, 29, 35, however, [sentence -> 0], []], [token, 37, 40, they, [sentence -> 0], []], [token, 42, 45, dont, [sentence -> 0], []], [token, 48, 54, support, [sentence -> 0], []], [token, 56, 59, each, [sentence -> 0], []], [token, 61, 65, other, [sentence -> 0], []], [token, 67, 70, that, [sentence -> 0], []], [token, 72, 75, much, [sentence -> 0], []]]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |[[token, 0, 4, Lucas, [sentence -> 0], []], [token, 6, 10, Nogal, [sentence -> 0], []], [token, 12, 21, Dunbercker, [sentence -> 0], []], [token, 23, 24, is, [sentence -> 0], []], [token, 26, 27, no, [sentence -> 0], []], [token, 29, 34, longer, [sentence -> 0], []], [token, 36, 40, happy, [sentence -> 0], []], [token, 41, 41, ., [sentence -> 0], []], [token, 43, 44, He, [sentence -> 0], []], [token, 46, 48, has, [sentence -> 0], []], [token, 50, 50, a, [sentence -> 0], []], [token, 52, 55, good, [sentence -> 0], []], [token, 57, 59, car, [sentence -> 0], []], [token, 61, 66, though, [sentence -> 0], []], [token, 67, 67, ., [sentence -> 0], []]]       |[[token, 0, 4, lucas, [sentence -> 0], []], [token, 6, 10, nogal, [sentence -> 0], []], [token, 12, 21, dunbercker, [sentence -> 0], []], [token, 23, 24, is, [sentence -> 0], []], [token, 26, 27, no, [sentence -> 0], []], [token, 29, 34, longer, [sentence -> 0], []], [token, 36, 40, happy, [sentence -> 0], []], [token, 43, 44, he, [sentence -> 0], []], [token, 46, 48, has, [sentence -> 0], []], [token, 50, 50, a, [sentence -> 0], []], [token, 52, 55, good, [sentence -> 0], []], [token, 57, 59, car, [sentence -> 0], []], [token, 61, 66, though, [sentence -> 0], []]]      |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |[[token, 0, 5, Europe, [sentence -> 0], []], [token, 7, 8, is, [sentence -> 0], []], [token, 10, 13, very, [sentence -> 0], []], [token, 15, 21, culture, [sentence -> 0], []], [token, 23, 26, rich, [sentence -> 0], []], [token, 27, 27, ., [sentence -> 0], []], [token, 29, 33, There, [sentence -> 0], []], [token, 35, 37, are, [sentence -> 0], []], [token, 39, 42, huge, [sentence -> 0], []], [token, 44, 51, churches, [sentence -> 0], []], [token, 52, 52, !, [sentence -> 0], []], [token, 54, 56, and, [sentence -> 0], []], [token, 58, 60, big, [sentence -> 0], []], [token, 62, 67, houses, [sentence -> 0], []], [token, 68, 68, !, [sentence -> 0], []]]      |[[token, 0, 5, europe, [sentence -> 0], []], [token, 7, 8, is, [sentence -> 0], []], [token, 10, 13, very, [sentence -> 0], []], [token, 15, 21, culture, [sentence -> 0], []], [token, 23, 26, rich, [sentence -> 0], []], [token, 29, 33, there, [sentence -> 0], []], [token, 35, 37, are, [sentence -> 0], []], [token, 39, 42, huge, [sentence -> 0], []], [token, 44, 51, churches, [sentence -> 0], []], [token, 54, 56, and, [sentence -> 0], []], [token, 58, 60, big, [sentence -> 0], []], [token, 62, 67, houses, [sentence -> 0], []]]                                              |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9oc0ws86HfC",
        "outputId": "f5e07a72-1dc2-44b6-e73c-94da6883e799"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(token=[Row(annotatorType='token', begin=0, end=4, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(token=[Row(annotatorType='token', begin=0, end=1, result='My', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='Russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=37, end=37, result='.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(token=[Row(annotatorType='token', begin=0, end=3, result='John', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='However', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=42, end=46, result=\"don't\", metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=76, end=76, result='.', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('normalized.result').take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLDV4A-S6Lrh",
        "outputId": "bcd101da-e78f-435d-b015-1d8ea585011f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['peter', 'is', 'a', 'very', 'good', 'person']),\n",
              " Row(result=['my', 'life', 'in', 'russia', 'is', 'very', 'interesting']),\n",
              " Row(result=['john', 'and', 'peter', 'are', 'brothers', 'however', 'they', 'dont', 'support', 'each', 'other', 'that', 'much']),\n",
              " Row(result=['lucas', 'nogal', 'dunbercker', 'is', 'no', 'longer', 'happy', 'he', 'has', 'a', 'good', 'car', 'though']),\n",
              " Row(result=['europe', 'is', 'very', 'culture', 'rich', 'there', 'are', 'huge', 'churches', 'and', 'big', 'houses'])]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('normalized').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vziB8Tm6PmQ",
        "outputId": "5c54d095-7c12-467c-9f8a-5eeee1d14dbd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(normalized=[Row(annotatorType='token', begin=0, end=4, result='peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(normalized=[Row(annotatorType='token', begin=0, end=1, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(normalized=[Row(annotatorType='token', begin=0, end=3, result='john', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='however', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=42, end=45, result='dont', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Normalizer"
      ],
      "metadata": {
        "id": "q8HRkZXz7AHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\n",
        "    THE WORLD'S LARGEST WEB DEVELOPER SITE\n",
        "    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\n",
        "    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\n",
        "  </div>\n",
        "\n",
        "</div>'''\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG-GpoRr65uL",
        "outputId": "6e1522d4-24c2-4f33-b4f1-b499a058d405"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|\n",
            "  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\n",
            "    THE WORLD'S LARGEST WEB DEVELOPER SITE\n",
            "    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\n",
            "    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\n",
            "  </div>\n",
            "\n",
            "</div>|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\")\n",
        "\n",
        "documentNormalizer.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVTKp2xn7IS_",
        "outputId": "e098a403-dfb1-4cdd-abda-4b7ff9aff192"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='DocumentNormalizer_59f36f35ce17', name='action', doc='action to perform applying regex patterns on text'): 'clean_up',\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='encoding', doc='file encoding to apply on normalized documents'): 'UTF-8',\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='lowercase', doc='whether to convert strings to lowercase'): False,\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='outputCol', doc='output annotation column. can be left default.'): 'normalizedDocument',\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='patterns', doc='normalization regex patterns which match will be removed from document. Defaults is <[^>]*>'): ['<[^>]*>'],\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='policy', doc='policy to remove pattern from text'): 'pretty_all',\n",
              " Param(parent='DocumentNormalizer_59f36f35ce17', name='replacement', doc='replacement string to apply when regexes match'): ' '}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "#default\n",
        "cleanUpPatterns = [\"<[^>]*>\"]\n",
        "\n",
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\") \\\n",
        "    .setAction(\"clean\") \\\n",
        "    .setPatterns(cleanUpPatterns) \\\n",
        "    .setReplacement(\" \") \\\n",
        "    .setPolicy(\"pretty_all\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "docPatternRemoverPipeline = \\\n",
        "  Pipeline() \\\n",
        "    .setStages([\n",
        "        documentAssembler,\n",
        "        documentNormalizer])\n",
        "    \n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = docPatternRemoverPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "LQpKSm_sATqj"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.select('normalizedDocument.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcGQxExiAvMB",
        "outputId": "f981dbd6-a5fe-4ece-8118-0e1534a05bb9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ the world's largest web developer site the world's largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords Cleaner"
      ],
      "metadata": {
        "id": "8znIa5zBBBsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"token\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "      #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)\n",
        "      \n",
        "stopwords_cleaner.getStopWords()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW1LpxsLAx_Q",
        "outputId": "0cb09e45-6230-4b9c-80df-e5b984d5d7fb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " 'should',\n",
              " 'now',\n",
              " \"i'll\",\n",
              " \"you'll\",\n",
              " \"he'll\",\n",
              " \"she'll\",\n",
              " \"we'll\",\n",
              " \"they'll\",\n",
              " \"i'd\",\n",
              " \"you'd\",\n",
              " \"he'd\",\n",
              " \"she'd\",\n",
              " \"we'd\",\n",
              " \"they'd\",\n",
              " \"i'm\",\n",
              " \"you're\",\n",
              " \"he's\",\n",
              " \"she's\",\n",
              " \"it's\",\n",
              " \"we're\",\n",
              " \"they're\",\n",
              " \"i've\",\n",
              " \"we've\",\n",
              " \"you've\",\n",
              " \"they've\",\n",
              " \"isn't\",\n",
              " \"aren't\",\n",
              " \"wasn't\",\n",
              " \"weren't\",\n",
              " \"haven't\",\n",
              " \"hasn't\",\n",
              " \"hadn't\",\n",
              " \"don't\",\n",
              " \"doesn't\",\n",
              " \"didn't\",\n",
              " \"won't\",\n",
              " \"wouldn't\",\n",
              " \"shan't\",\n",
              " \"shouldn't\",\n",
              " \"mustn't\",\n",
              " \"can't\",\n",
              " \"couldn't\",\n",
              " 'cannot',\n",
              " 'could',\n",
              " \"here's\",\n",
              " \"how's\",\n",
              " \"let's\",\n",
              " 'ought',\n",
              " \"that's\",\n",
              " \"there's\",\n",
              " \"what's\",\n",
              " \"when's\",\n",
              " \"where's\",\n",
              " \"who's\",\n",
              " \"why's\",\n",
              " 'would']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stopwords_cleaner\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "-e3u9zElBSNb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyDv4g7CBaaI",
        "outputId": "9f52207c-5f5f-4d47-c5fa-9f7a788ab648"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|         cleanTokens|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 3, 6, li...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('cleanTokens.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXAPe4lWBfrP",
        "outputId": "d1a0aef2-64ba-4491-e402-4e76e57712cc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'good', 'person', '.'])]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Assembler"
      ],
      "metadata": {
        "id": "TA2DweRwBn9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector().\\\n",
        "    setInputCols(['document']).\\\n",
        "    setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentences\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(False)\\\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "\n",
        "tokenassembler = TokenAssembler()\\\n",
        "    .setInputCols([\"sentences\", \"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"clean_text\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "     documentAssembler,\n",
        "    sentenceDetector,\n",
        "     tokenizer,\n",
        "     normalizer,\n",
        "     stopwords_cleaner,\n",
        "     tokenassembler\n",
        " ])\n",
        "\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrZJmUKpBmP_",
        "outputId": "4d6318ad-66fc-4e11-d431-0b75b21a4806"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|           sentences|               token|          normalized|         cleanTokens|          clean_text|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[document, 0, 16...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, My...|[[token, 3, 6, li...|[[document, 0, 22...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[document, 0, 27...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[document, 0, 18...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[document, 0, 41...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[document, 0, 34...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[document, 0, 27...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[document, 0, 18...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('clean_text').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Ze2UHHGjoe",
        "outputId": "756c59de-4dc5-4663-a997-8be0518beb60"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(clean_text=[Row(annotatorType='document', begin=0, end=16, result='Peter good person', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we use TokenAssembler().setPreservePosition(True), the original borders will be preserved (dropped & unwanted chars will be replaced by spaces)\n",
        "\n",
        "result.select('clean_text').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RfGh26sGpk-",
        "outputId": "a4da44fe-ef45-44d2-b05e-e099513a4665"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(clean_text=[Row(annotatorType='document', begin=0, end=16, result='Peter good person', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('text', F.explode('clean_text.result').alias('clean_text')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEBlNeiPHDSN",
        "outputId": "9d311d99-fed4-4e7d-f573-9936ec5f0b8d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "|text                                                                         |clean_text                         |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "|Peter is a very good person.                                                 |Peter good person                  |\n",
            "|My life in Russia is very interesting.                                       |life Russia interesting            |\n",
            "|John and Peter are brothers. However they don't support each other that much.|John Peter brothers                |\n",
            "|John and Peter are brothers. However they don't support each other that much.|However dont support much          |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |Lucas Nogal Dunbercker longer happy|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |good car though                    |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |Europe culture rich                |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |huge churches                      |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |big houses                         |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result.withColumn(\n",
        "    \"tmp\", \n",
        "    F.explode(\"clean_text\")) \\\n",
        "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFTSCkjqHF-O",
        "outputId": "d79314df-a3be-4243-cf48-957b30ca314d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----------------------------------+--------+\n",
            "|begin|end|result                             |sentence|\n",
            "+-----+---+-----------------------------------+--------+\n",
            "|0    |16 |Peter good person                  |0       |\n",
            "|0    |22 |life Russia interesting            |0       |\n",
            "|0    |18 |John Peter brothers                |0       |\n",
            "|29   |53 |However dont support much          |1       |\n",
            "|0    |34 |Lucas Nogal Dunbercker longer happy|0       |\n",
            "|43   |57 |good car though                    |1       |\n",
            "|0    |18 |Europe culture rich                |0       |\n",
            "|29   |41 |huge churches                      |1       |\n",
            "|54   |63 |big houses                         |2       |\n",
            "+-----+---+-----------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we hadn't used Sentence Detector, this would be what we got. (tokenizer gets document instead of sentences column)\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "tokenassembler = TokenAssembler()\\\n",
        "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"clean_text\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "     documentAssembler,\n",
        "     tokenizer,\n",
        "     normalizer,\n",
        "     stopwords_cleaner,\n",
        "     tokenassembler\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.select('text', 'clean_text.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTz_JApJHPqQ",
        "outputId": "7024ec60-c8d4-4137-b54f-f9735fd8a2dc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|text                                                                         |result                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[Peter good person]                                  |\n",
            "|My life in Russia is very interesting.                                       |[life Russia interesting]                            |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[John Peter brothers However dont support much]      |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy good car though]|\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[Europe culture rich huge churches big houses]       |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result.withColumn(\n",
        "    \"tmp\", \n",
        "    F.explode(\"clean_text\")) \\\n",
        "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C82_P4vgIM0Z",
        "outputId": "9021c7f9-8e31-4fcc-e8f9-98a6fd4a1f4f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------------------------------------------------+--------+\n",
            "|begin|end|result                                             |sentence|\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "|0    |16 |Peter good person                                  |0       |\n",
            "|0    |22 |life Russia interesting                            |0       |\n",
            "|0    |44 |John Peter brothers However dont support much      |0       |\n",
            "|0    |50 |Lucas Nogal Dunbercker longer happy good car though|0       |\n",
            "|0    |43 |Europe culture rich huge churches big houses       |0       |\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**important note:**\n",
        "\n",
        "If you have some other steps & annotators in your pipeline that will need to use the tokens from cleaned text (assembled tokens), you will need to tokenize the processed text again as the original text is probably changed completely.\n",
        "\n",
        "### Stemmer"
      ],
      "metadata": {
        "id": "A1_SFrtZIeYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")"
      ],
      "metadata": {
        "id": "qkUd7WF7IWvq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "QoQ2n-YnIlVb"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU0DMMJIInth",
        "outputId": "964a11f8-6384-4139-ae3a-58f05f2a3f48"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('stem.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Btq1VvIp8I",
        "outputId": "13839c3c-e47d-43dd-fdcf-fbe75c7747fa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------+\n",
            "|result                                                                                     |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "|[peter, i, a, veri, good, person, .]                                                       |\n",
            "|[my, life, in, russia, i, veri, interest, .]                                               |\n",
            "|[john, and, peter, ar, brother, ., howev, thei, don't, support, each, other, that, much, .]|\n",
            "|[luca, nogal, dunberck, i, no, longer, happi, ., he, ha, a, good, car, though, .]          |\n",
            "|[europ, i, veri, cultur, rich, ., there, ar, huge, church, !, and, big, hous, !]           |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pyspark.sql.functions as F\n",
        "\n",
        "#result_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result')).alias(\"cols\")) \\\n",
        "#.select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "#        F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n",
        "\n",
        "#result_df.head(10)"
      ],
      "metadata": {
        "id": "2BCB--ehIuCz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatizer\n",
        "\n",
        "Retrieves lemmas out of words with the objective of returning a base dictionary word "
      ],
      "metadata": {
        "id": "shmJ-INX54tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt"
      ],
      "metadata": {
        "id": "B6G2_YZtLcxi"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\") \\\n",
        "    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")"
      ],
      "metadata": {
        "id": "xvMqxrqHI0Pe"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.extractParamMap()"
      ],
      "metadata": {
        "id": "nefmw0s76NeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de12d40-f71d-4065-f1fd-89cd146fb5ef"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='Lemmatizer_1d54ce757f86', name='dictionary', doc=\"lemmatizer external dictionary. needs 'keyDelimiter' and 'valueDelimiter' in options for parsing target text\"): JavaObject id=o2573,\n",
              " Param(parent='Lemmatizer_1d54ce757f86', name='inputCols', doc='previous annotations columns, if renamed'): ['token'],\n",
              " Param(parent='Lemmatizer_1d54ce757f86', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='Lemmatizer_1d54ce757f86', name='outputCol', doc='output annotation column. can be left default.'): 'lemma'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer,\n",
        " lemmatizer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "id": "lK-wJbR07FPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e31e82-46f5-46ae-a555-8ebec48b958f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|               lemma|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|[[token, 0, 1, My...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('lemma.result').show(truncate = False)"
      ],
      "metadata": {
        "id": "3_03CKYe7MTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc10f8b-1801-4075-8816-ac5411dbc142"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------+\n",
            "|result                                                                                       |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "|[Peter, be, a, very, good, person, .]                                                        |\n",
            "|[My, life, in, Russia, be, very, interest, .]                                                |\n",
            "|[John, and, Peter, be, brother, ., However, they, don't, support, each, other, that, much, .]|\n",
            "|[Lucas, Nogal, Dunbercker, be, no, long, happy, ., He, have, a, good, car, though, .]        |\n",
            "|[Europe, be, very, culture, rich, ., There, be, huge, church, !, and, big, house, !]         |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#result_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result',  'lemma.result')).alias(\"cols\")) \\\n",
        "#.select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "#        F.expr(\"cols['1']\").alias(\"stem\"),\n",
        "#        F.expr(\"cols['2']\").alias(\"lemma\")).toPandas()\n",
        "\n",
        "#result_df.head(10)"
      ],
      "metadata": {
        "id": "vJ5hgREmNAm_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fe7FyCGEd4eT"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGram Generator"
      ],
      "metadata": {
        "id": "hKwR-aX8NPPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ngrams_cum = NGramGenerator() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"ngrams\") \\\n",
        "            .setN(3) \\\n",
        "            .setEnableCumulative(True)\\\n",
        "            .setDelimiter(\"_\") # Default is space\n",
        "    \n",
        "# .setN(3) means, take bigrams and trigrams.\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " ngrams_cum\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.select('ngrams.result').show(truncate=200)"
      ],
      "metadata": {
        "id": "TBhRA_sJNHGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0287068-d0c7-4679-e48e-a51cbd8c4ef8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                  result|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                    [Peter, is, a, very, good, person, ., Peter_is, is_a, a_very, very_good, good_person, person_., Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
            "|[My, life, in, Russia, is, very, interesting, ., My_life, life_in, in_Russia, Russia_is, is_very, very_interesting, interesting_., My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_...|\n",
            "|[John, and, Peter, are, brothers, ., However, they, don't, support, each, other, that, much, ., John_and, and_Peter, Peter_are, are_brothers, brothers_., ._However, However_they, they_don't, don't_...|\n",
            "|[Lucas, Nogal, Dunbercker, is, no, longer, happy, ., He, has, a, good, car, though, ., Lucas_Nogal, Nogal_Dunbercker, Dunbercker_is, is_no, no_longer, longer_happy, happy_., ._He, He_has, has_a, a_...|\n",
            "|[Europe, is, very, culture, rich, ., There, are, huge, churches, !, and, big, houses, !, Europe_is, is_very, very_culture, culture_rich, rich_., ._There, There_are, are_huge, huge_churches, churche...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_nonCum = NGramGenerator() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"ngrams_v2\") \\\n",
        "            .setN(3) \\\n",
        "            .setEnableCumulative(False)\\\n",
        "            .setDelimiter(\"_\") # Default is space\n",
        "    \n",
        "ngrams_nonCum.transform(result).select('ngrams_v2.result').show(truncate=200)"
      ],
      "metadata": {
        "id": "tlxTI7YhNXVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e8370d-871b-4857-b1ea-087611b45ec6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                  result|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                   [Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
            "|                                                                                                     [My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_interesting, very_interesting_.]|\n",
            "|[John_and_Peter, and_Peter_are, Peter_are_brothers, are_brothers_., brothers_._However, ._However_they, However_they_don't, they_don't_support, don't_support_each, support_each_other, each_other_th...|\n",
            "|   [Lucas_Nogal_Dunbercker, Nogal_Dunbercker_is, Dunbercker_is_no, is_no_longer, no_longer_happy, longer_happy_., happy_._He, ._He_has, He_has_a, has_a_good, a_good_car, good_car_though, car_though_.]|\n",
            "|[Europe_is_very, is_very_culture, very_culture_rich, culture_rich_., rich_._There, ._There_are, There_are_huge, are_huge_churches, huge_churches_!, churches_!_and, !_and_big, and_big_houses, big_ho...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Matcher"
      ],
      "metadata": {
        "id": "sry8PkbjNzDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"matched_entities\")\\\n",
        "\n",
        "entity_extractor.extractParamMap()"
      ],
      "metadata": {
        "id": "ek29k0GZNeRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f192eb-dd7c-4b97-a7fa-2eb743ace7b9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='TextMatcher_920c710729f3', name='caseSensitive', doc='whether to match regardless of case. Defaults true'): True,\n",
              " Param(parent='TextMatcher_920c710729f3', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n",
              "  'token'],\n",
              " Param(parent='TextMatcher_920c710729f3', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='TextMatcher_920c710729f3', name='mergeOverlapping', doc='whether to merge overlapping matched chunks. Defaults false'): False,\n",
              " Param(parent='TextMatcher_920c710729f3', name='outputCol', doc='output annotation column. can be left default.'): 'matched_entities'}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
        "\n",
        "news_df = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"news_category_train.csv\")\n"
      ],
      "metadata": {
        "id": "TNQrDTZtN53p"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.show(5, truncate=50)"
      ],
      "metadata": {
        "id": "Q_L8rvdMOAXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400bb18e-2c8e-4b9f-c05c-df9b861d627c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business| Short sellers, Wall Street's dwindling band of...|\n",
            "|Business| Private investment firm Carlyle Group, which h...|\n",
            "|Business| Soaring crude prices plus worries about the ec...|\n",
            "|Business| Authorities have halted oil export flows from ...|\n",
            "|Business| Tearaway world oil prices, toppling records an...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write the target entities to txt file \n",
        "\n",
        "entities = ['Wall Street', 'USD', 'stock', 'NYSE']\n",
        "with open ('financial_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')\n",
        "\n",
        "\n",
        "entities = ['soccer', 'world cup', 'Messi', 'FC Barcelona']\n",
        "with open ('sport_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')"
      ],
      "metadata": {
        "id": "vuJfFAk5OFLd"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"description\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "financial_entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"financial_entities\")\\\n",
        "    .setEntities(\"financial_entities.txt\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setEntityValue('financial_entity')\n",
        "\n",
        "sport_entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"sport_entities\")\\\n",
        "    .setEntities(\"sport_entities.txt\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setEntityValue('sport_entity')\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " financial_entity_extractor,\n",
        " sport_entity_extractor\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"description\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "HfhNMsuLPgHn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(news_df)"
      ],
      "metadata": {
        "id": "1oVHIHq2P030"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('financial_entities.result','sport_entities.result').take(2)"
      ],
      "metadata": {
        "id": "tgZBB2PEP4Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a597b2e1-ac22-46c7-f740-0a32d9d11fb1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=[], result=[]), Row(result=[], result=[])]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('description','financial_entities.result','sport_entities.result')\\\n",
        ".toDF('text','financial_matches','sport_matches').filter((F.size('financial_matches')>1) | (F.size('sport_matches')>1))\\\n",
        ".show(truncate=70)\n"
      ],
      "metadata": {
        "id": "owp_YOjCQVEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45b3647-89b3-4da2-eecf-b9d48358b487"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|                                                                  text|                 financial_matches|      sport_matches|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|\"Company launched the biggest electronic auction of stock in Wall S...|              [stock, Wall Street]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price for its ini...|                    [stock, stock]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price this mornin...|                    [stock, stock]|                 []|\n",
            "| Shares of Air Canada  (AC.TO) fell by more than half on Wednesday,...|                    [Stock, stock]|                 []|\n",
            "|Stock prices are lower in moderate trading. The Dow Jones Industria...|                    [Stock, Stock]|                 []|\n",
            "|The bad news just keeps pouring in for mutual fund manager Janus Ca...|                      [NYSE, NYSE]|                 []|\n",
            "|  Shaun Wright Phillips scored in his international debut as Englan...|                                []|[soccer, World Cup]|\n",
            "|NEWCASTLE, ENGLAND - England deservedly beat Ukraine 3-0 today in t...|                                []|[soccer, World Cup]|\n",
            "|MONTREAL (Reuters) - Shares of Air Canada (AC.TO: Quote, Profile, R...|                    [Stock, stock]|                 []|\n",
            "|\"SAN JOSE, California - On the cusp of its voyage into public tradi...|[stock, Wall Street, stock, Stock]|                 []|\n",
            "|\"Shortly before noon today, Google Inc. stock began trading under t...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: EA to take World Cup soccer to Xbox...IBM chalks up t...|                                []|[World Cup, soccer]|\n",
            "|The U.S. Securities and Exchange Commission yesterday approved Goog...|                    [stock, stock]|                 []|\n",
            "|After a bumpy ride toward becoming a publicly traded company, Googl...|                    [stock, stock]|                 []|\n",
            "|In the most highly anticipated Wall Street debut since the heady da...|              [Wall Street, stock]|                 []|\n",
            "|NEW YORK Despite voluble skepticism among investors, Google #39;s s...|                    [stock, stock]|                 []|\n",
            "|If only the rest of my investments worked out this way. One week ag...|                    [stock, stock]|                 []|\n",
            "| U.S. stocks to watch: GOOGLE INC. (GOOG.O) Google shares jumped 18...|                    [stock, stock]|                 []|\n",
            "|\" U.S. stocks to watch: GOOGLE INC.  &lt;A HREF=\"\"http://www.invest...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: KDE updates Linux desktop...EA to take World Cup socc...|                                []|[World Cup, soccer]|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#result_df = result.select(F.explode(F.arrays_zip('financial_entities.result', 'financial_entities.begin',  'financial_entities.end')).alias(\"cols\")) \\\n",
        "#.select(F.expr(\"cols['0']\").alias(\"clinical_entities\"),\n",
        "#        F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "#        F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
        "\n",
        "#result_df.head(10)"
      ],
      "metadata": {
        "id": "XiQU-n0CSRbO"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegEx Matcher"
      ],
      "metadata": {
        "id": "CgDKLfy2Sqlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q\thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\n",
        "\n",
        "pubMedDF = spark.read\\\n",
        "                .option(\"header\", \"true\")\\\n",
        "                .csv(\"./pubmed-sample.csv\")\\\n",
        "                .filter(\"AB IS NOT null\")\\\n",
        "                .withColumnRenamed(\"AB\", \"text\")\\\n",
        "                .drop(\"TI\")\n",
        "\n",
        "pubMedDF.show(truncate=50)"
      ],
      "metadata": {
        "id": "ESqQ3l8VSjH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ee352b-c546-4308-9e0f-1f5caf5aa2b0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+\n",
            "|                                              text|\n",
            "+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|\n",
            "|BACKGROUND: At present, it is one of the most i...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|\n",
            "|Kohlschutter syndrome is a rare neurodegenerati...|\n",
            "|Statistical analysis of neuroimages is commonly...|\n",
            "|The synthetic DOX-LNA conjugate was characteriz...|\n",
            "|Our objective was to compare three different me...|\n",
            "|We conducted a phase II study to assess the eff...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|\n",
            "|We presented the tachinid fly Exorista japonica...|\n",
            "|The literature dealing with the water conductin...|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|\n",
            "|The localizing and lateralizing values of eye a...|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|\n",
            "|For the construction of new combinatorial libra...|\n",
            "|We report the results of a screen for genetic a...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|\n",
            "|It is known that patients with Klinefelter's sy...|\n",
            "+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules = '''\n",
        "renal\\s\\w+, started with 'renal'\n",
        "cardiac\\s\\w+, started with 'cardiac'\n",
        "\\w*ly\\b, ending with 'ly'\n",
        "\\S*\\d+\\S*, match any word that contains numbers\n",
        "(\\d+).?(\\d*)\\s*(mg|ml|g), match medication metrics\n",
        "'''\n",
        "\n",
        "with open('regex_rules.txt', 'w') as f:\n",
        "    \n",
        "    f.write(rules)"
      ],
      "metadata": {
        "id": "-dX6jhY8T2YK"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RegexMatcher().extractParamMap()"
      ],
      "metadata": {
        "id": "fdB85DdvUMLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7178c3e6-5d0a-4f25-d02f-f66d12a251a4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='RegexMatcher_4a7ee8ff5b57', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='RegexMatcher_4a7ee8ff5b57', name='strategy', doc='MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE'): 'MATCH_ALL'}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "regex_matcher = RegexMatcher()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setStrategy(\"MATCH_ALL\")\\\n",
        "    .setOutputCol(\"regex_matches\")\\\n",
        "    .setExternalRules(path='./regex_rules.txt', delimiter=',')\n",
        "    \n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " regex_matcher\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "match_df = pipelineModel.transform(pubMedDF)\n",
        "\n",
        "match_df.select('regex_matches.result').take(3)"
      ],
      "metadata": {
        "id": "S8ptR3NpUXAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c2a22e-230c-4454-a2da-092415e6cd60"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['inwardly', 'family', 'spansapproximately', 'byapproximately', 'approximately', 'respectively', 'poly', 'KCNJ9', '3.3,', 'GIRK3)', 'KCNJ9', '1q21-23', '7.6', '2.2', '2.6', 'identified14', 'aVal366Ala', '8', 'KCNJ9', 'KCNJ9', '9 g']),\n",
              " Row(result=['previously', 'previously', 'intravenously', 'previously', '25', 'mg/m(2)', '1', '8', 'a3', '50', '20.0%', '(10', '50;', '95%', 'interval,10.0-33.7%).', '58.0%', '[10', '18', '50].', '(50%', '115.0', '17.3%', '52).', '25 mg']),\n",
              " Row(result=['renal failure', 'cardiac surgery', 'cardiac surgery', 'cardiac surgical', 'early', 'statistically', 'analy', '1995', '2005', '=9796).', '2.9', '11years).', '11.3%', '1105),', '7.2%', '30%', '0.0001),', '1.55,95%', '1.42-1.70,', '0.0001).'])]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.select('text','regex_matches.result')\\\n",
        ".toDF('text','matches').filter(F.size('matches')>1)\\\n",
        ".show(truncate=70)"
      ],
      "metadata": {
        "id": "Y3qaCBjWcW86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762a5a4d-20d1-4f53-b983-63d09b155e71"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|                                                                  text|                                                               matches|\n",
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activ...|[inwardly, family, spansapproximately, byapproximately, approximate...|\n",
            "|BACKGROUND: At present, it is one of the most important issues for ...|[previously, previously, intravenously, previously, 25, mg/m(2), 1,...|\n",
            "|OBJECTIVE: To investigate the relationship between preoperative atr...|[renal failure, cardiac surgery, cardiac surgery, cardiac surgical,...|\n",
            "|Combined EEG/fMRI recording has been used to localize the generator...|[normally, significantly, effectively, analy, only, considerably, 2...|\n",
            "|Statistical analysis of neuroimages is commonly approached with int...|[analy, commonly, overly, normally, thatsuccessfully, recently, ana...|\n",
            "|The synthetic DOX-LNA conjugate was characterized by proton nuclear...|                                             [wasanaly, substantially]|\n",
            "|Our objective was to compare three different methods of blood press...|[daily, only, Conversely, Hourly, hourly, Hourly, hourly, hourly, h...|\n",
            "|We conducted a phase II study to assess the efficacy and tolerabili...|[analy, respectively, generally, 5-fluorouracil, (5-FU)-, 5-FU-base...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoenzyme that catalyzes...|[cataly, methylgly, gly, ethylgly, dimethylgly, spectrally, practic...|\n",
            "|We presented the tachinid fly Exorista japonica with moving host mo...|                                             [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conducting properties of sapw...|                               [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopropyl-5'-O-d4T monoph...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d4T, Chitosan-d4T, 1....|\n",
            "|An HPLC-ESI-MS-MS method has been developed for the quantitative de...|[chromatographically, respectively, successfully, C18, (n=5), 95.0%...|\n",
            "|The localizing and lateralizing values of eye and head ictal deviat...|                                                        [early, early]|\n",
            "|OBJECTIVE: To evaluate the effectiveness and acceptability of expec...|[weekly, respectively, theanaly, 2006, 2007,, 2, 66, 1), 30patients...|\n",
            "|We report the results of a screen for genetic association with urin...|[poly, threepoly, significantly, analy, actually, anextremely, only...|\n",
            "|Intraparenchymal pericatheter cyst is rarely reported. Obstruction ...|                                  [rarely, possibly, unusually, Early]|\n",
            "|PURPOSE: To compare the effectiveness, potential advantages and com...|[analy, comparatively, wassignificantly, respectively, a7-year, 155...|\n",
            "|We have demonstrated a new type of all-optical 2 x 2 switch by usin...|[approximately, fully, approximately, approximately, approximately,...|\n",
            "|Physalis peruviana (PP) is a widely used medicinal herb for treatin...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82.3%), onFeCl2-ascorb...|\n",
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Date Matcher"
      ],
      "metadata": {
        "id": "TBPvO4XCdawu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MultiDateMatcher().extractParamMap()"
      ],
      "metadata": {
        "id": "PjL4mPv6dVtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0798b3-6fb9-41b2-be38-5c576cbfdc6e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='MultiDateMatcher_4f4f8929861c', name='dateFormat', doc='desired format for dates extracted'): 'yyyy/MM/dd',\n",
              " Param(parent='MultiDateMatcher_4f4f8929861c', name='defaultDayWhenMissing', doc='which day to set when it is missing from parsed input'): 1,\n",
              " Param(parent='MultiDateMatcher_4f4f8929861c', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='MultiDateMatcher_4f4f8929861c', name='readMonthFirst', doc='Whether to parse july 07/05/2015 or as 05/07/2015'): True}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "date_matcher = MultiDateMatcher() \\\n",
        "    .setInputCols('document') \\\n",
        "    .setOutputCol(\"date\") \\\n",
        "    .setDateFormat(\"yyyy/MM/dd\")\n",
        "        \n",
        "date_pipeline = PipelineModel(stages=[\n",
        " documentAssembler, \n",
        " date_matcher\n",
        " ])\n",
        "\n",
        "sample_df = spark.createDataFrame([['I saw him yesterday and he told me that he will visit us next week']]).toDF(\"text\")\n",
        "\n",
        "result = date_pipeline.transform(sample_df)\n",
        "\n",
        "result.select('date.result').show(truncate=False)"
      ],
      "metadata": {
        "id": "r0Bcz6XKdfu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92898c65-4b24-43b3-8d51-dabfc23fd9ee"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+\n",
            "|result                  |\n",
            "+------------------------+\n",
            "|[2022/04/06, 2022/03/29]|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning with UDF"
      ],
      "metadata": {
        "id": "rGmztWJJeCxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
        "\n",
        "text_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "import re\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "clean_text = lambda s: re.sub(r'<[^>]*>', '', s)\n",
        "\n",
        "text_df.withColumn('cleaned', udf(clean_text, StringType())('text')).select('text','cleaned').show(truncate= False)"
      ],
      "metadata": {
        "id": "8akZ49CtdkOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4ce595-5676-4923-ae21-f8cb5a50ee0b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "|text                                                                                          |cleaned                |\n",
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|Have a great birth day!|\n",
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_not_alnum_count = lambda s: len([i for i in s if not i.isalnum() and i!=' '])\n",
        "\n",
        "find_not_alnum_count(\"it's your birth day!\")"
      ],
      "metadata": {
        "id": "JG1uQaX9fhGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75967067-7775-4d44-eee4-0310e680ccb9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
        "\n",
        "find_not_alnum_count(text)"
      ],
      "metadata": {
        "id": "qCEDQG4in7jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671b4ea8-edaf-48c4-ca77-03d823d7d70c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_df.withColumn('cleaned', udf(find_not_alnum_count, IntegerType())('text')).select('text','cleaned').show(truncate= False)"
      ],
      "metadata": {
        "id": "y4FssVxBn_Hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05f6c1e-6a19-440b-ec45-d3625cbc1ac8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "|text                                                                                          |cleaned|\n",
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|23     |\n",
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FINISHER\n",
        "\n",
        "***Finisher:*** Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n",
        "\n",
        "If we just want the desired output column in the final dataframe, we can use Finisher to drop previous stages in the final output and get the `result` from the process.\n",
        "\n",
        "This is very handy when you want to use the output from Spark NLP annotator as an input to another Spark ML transformer.\n",
        "\n",
        "Settable parameters are:\n",
        "\n",
        "`setInputCols()`\n",
        "\n",
        "`setOutputCols()`\n",
        "\n",
        "`setCleanAnnotations(True)` -> Whether to remove intermediate annotations\n",
        "\n",
        "`setValueSplitSymbol(“#”)` -> split values within an annotation character\n",
        "\n",
        "`setAnnotationSplitSymbol(“@”)` -> split values between annotations character\n",
        "\n",
        "`setIncludeMetadata(False)` -> Whether to include metadata keys. Sometimes useful in some annotations.\n",
        "\n",
        "`setOutputAsArray(False)` -> Whether to output as Array. Useful as input for other Spark transformers."
      ],
      "metadata": {
        "id": "mCKY9GF1o2F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"regex_matches\"]) \\\n",
        "    .setIncludeMetadata(False) # set to False to remove metadata\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " regex_matcher,\n",
        " finisher\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "match_df = pipelineModel.transform(pubMedDF)\n",
        "\n",
        "match_df.show(truncate = 50)"
      ],
      "metadata": {
        "id": "e9zZipfxovGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66089637-d00e-40f9-836a-c2d8c9cfec61"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
            "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
            "|Kohlschutter syndrome is a rare neurodegenerati...|                                          [family]|\n",
            "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
            "|The synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\n",
            "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
            "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
            "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
            "|The localizing and lateralizing values of eye a...|                                    [early, early]|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
            "|For the construction of new combinatorial libra...|                                           [newly]|\n",
            "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
            "|It is known that patients with Klinefelter's sy...|                                                []|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.printSchema()"
      ],
      "metadata": {
        "id": "bIDI0Vn8p6vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183e678f-e9b8-479f-bc82-18753c82bd5f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- finished_regex_matches: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.filter(F.size('finished_regex_matches')>2).show(truncate = 50)"
      ],
      "metadata": {
        "id": "yXNHLRBjqG1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8dbfb5-467a-4edb-eb94-9f8c17faefcd"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
            "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
            "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
            "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
            "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
            "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
            "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
            "|PURPOSE: To compare the effectiveness, potentia...|[analy, comparatively, wassignificantly, respec...|\n",
            "|We have demonstrated a new type of all-optical ...|[approximately, fully, approximately, approxima...|\n",
            "|Physalis peruviana (PP) is a widely used medici...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82...|\n",
            "|We report the discovery of a series of substitu...|[highly, potentially, highly, respectively, tub...|\n",
            "|The purpose of this study was to identify and c...|[family, Nearly, only, 43, 10, 44%, 32%, 64%, 4...|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIGHTPIPELINE\n",
        "\n",
        "LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n",
        "\n",
        "Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n",
        "\n",
        " **It is nearly 20x faster than using Spark ML Pipeline**\n",
        "\n",
        "`LightPipeline(someTrainedPipeline).annotate(someStringOrArray)`"
      ],
      "metadata": {
        "id": "Z63zaeIPqUIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer,\n",
        " lemmatizer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "pipelineModel.transform(spark_df).show()"
      ],
      "metadata": {
        "id": "eA6S5UzzqK0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9648b1-fc63-43e5-dcd6-ce8b9bd6c8b3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|               lemma|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|[[token, 0, 1, My...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "light_model = LightPipeline(pipelineModel)\n",
        "\n",
        "light_result = light_model.annotate(\"John and Peter are brothers. However they don't support each other that much.\")"
      ],
      "metadata": {
        "id": "Kud2TjLnqmXp"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_result.keys()"
      ],
      "metadata": {
        "id": "TJyfzRVIrl6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c25eb2e-0760-4bc4-d912-bae71ae1d656"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'token', 'stem', 'lemma'])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(light_result['token'], light_result['stem'], light_result['lemma']))"
      ],
      "metadata": {
        "id": "4Aw5vBjUrqDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36945230-8b51-4896-e6c8-a229f41c2054"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'john', 'John'),\n",
              " ('and', 'and', 'and'),\n",
              " ('Peter', 'peter', 'Peter'),\n",
              " ('are', 'ar', 'be'),\n",
              " ('brothers', 'brother', 'brother'),\n",
              " ('.', '.', '.'),\n",
              " ('However', 'howev', 'However'),\n",
              " ('they', 'thei', 'they'),\n",
              " (\"don't\", \"don't\", \"don't\"),\n",
              " ('support', 'support', 'support'),\n",
              " ('each', 'each', 'each'),\n",
              " ('other', 'other', 'other'),\n",
              " ('that', 'that', 'that'),\n",
              " ('much', 'much', 'much'),\n",
              " ('.', '.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "light_result = light_model.fullAnnotate(\"John and Peter are brothers. However they don't support each other that much.\")"
      ],
      "metadata": {
        "id": "M0jF2kTTr6dk"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_result"
      ],
      "metadata": {
        "id": "KfOcDFYpr-f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3868c8da-0631-480c-e5f8-6b2184f0948b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': [Annotation(document, 0, 76, John and Peter are brothers. However they don't support each other that much., {})],\n",
              "  'lemma': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, be, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
              "  'stem': [Annotation(token, 0, 3, john, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, ar, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, howev, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, thei, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
              "  'token': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, are, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brothers, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})]}]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list= [\"How did serfdom develop in and then leave Russia ?\",\n",
        "\"There will be some exciting breakthroughs in NLP this year.\"]\n",
        "\n",
        "light_model.annotate(text_list)"
      ],
      "metadata": {
        "id": "KY97gYqJsAxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb6b408-c558-42ae-9323-5480120386ec"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': ['How did serfdom develop in and then leave Russia ?'],\n",
              "  'lemma': ['How',\n",
              "   'do',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leave',\n",
              "   'Russia',\n",
              "   '?'],\n",
              "  'stem': ['how',\n",
              "   'did',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leav',\n",
              "   'russia',\n",
              "   '?'],\n",
              "  'token': ['How',\n",
              "   'did',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leave',\n",
              "   'Russia',\n",
              "   '?']},\n",
              " {'document': ['There will be some exciting breakthroughs in NLP this year.'],\n",
              "  'lemma': ['There',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'exciting',\n",
              "   'breakthrough',\n",
              "   'in',\n",
              "   'NLP',\n",
              "   'this',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'stem': ['there',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'excit',\n",
              "   'breakthrough',\n",
              "   'in',\n",
              "   'nlp',\n",
              "   'thi',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'token': ['There',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'exciting',\n",
              "   'breakthroughs',\n",
              "   'in',\n",
              "   'NLP',\n",
              "   'this',\n",
              "   'year',\n",
              "   '.']}]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**important note:** When you use Finisher in your pipeline, regardless of setting `cleanAnnotations` to False or True, LigtPipeline will only return the finished columns."
      ],
      "metadata": {
        "id": "GvmVJjKasMYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ouSZ30lssGQu"
      },
      "execution_count": 119,
      "outputs": []
    }
  ]
}